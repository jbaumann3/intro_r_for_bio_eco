{
  "hash": "c60b2452c3f8d12faf1d1c50f7bad19f",
  "result": {
    "markdown": "---\ntitle: \"Lab_5_correlation_regression_t_chi\"\nauthor: \"Justin Baumann\"\nformat: \n  html:\n    toc: true\n  pdf:\n    toc: true\n    number-sections: true\n    colorlinks: true\n    self-contained: true\n\neditor: visual\n---\n\n\n# **Learning Objectives**\n\n1.) Explore correlations between numerical variables (two variables and many)\\\n2.) Practice running and interpreting linear regressions\\\n3.) Calculate and interpret R\\^2 values\\\n4.) Test assumptions of linear regression\\\n5.) practice running and interpreting t-tests\\\n6.) test assumptions of t-tests\\\n7.) Practice chi-square tests and understand their application\\\n \n\n# **1: Load packages**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(see)\nlibrary(car)\nlibrary(patchwork)\nlibrary(ggsci)\nlibrary(ggridges)\nlibrary(performance)\nlibrary(Hmisc) #for correlation matrix\nlibrary(corrplot)#to visualize correlation matrices\nlibrary(car) #contains some statistical tests we need to assess assumptions\n```\n:::\n\n\n# **2: Correlation between numerical variables**\n\n::: panel-tabset\nOften in science, it can be useful to assess the correlation between numerical variables (how does a change in one variable impact a change in another?). We may use these correlations to tell us which variables to include or exclude from more complex models and we can also use these correlations to understand relationships between variables and thus, possibly search for mechanisms to explain said relationships.\n\n## Correlation Coefficients\n\nA **correlation coefficient (r)** tells us the relationship (strength and direction) between two variables. These coefficients can be positive or negative and will range from 0 to 1 (or negative 1). Values nearer to 1 (or negative 1) indicate stronger correlations and values closer to 0 indicate weaker correlations\n\nLet's try out some correlations using the iris data.\\\nIs there a correlation between sepal length and sepal width? Let's test each species separately for now.\\\n**Step 1: make a scatterplot**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#filter down to a single species\nvirg<-iris %>%\n  filter(Species=='virginica')\n\n#make a plot\nggplot(virg, aes(x=Sepal.Length, y=Sepal.Width))+\n  geom_point()+\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\\\n**Step 2: Calculate a correlation coeficient (r)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(virg$Sepal.Length, virg$Sepal.Width)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4572278\n```\n:::\n:::\n\n\nThis value (r=0.45) positive and middle of the road/strong. This tells us that some correlation likely exists.\n\n**Step 3: Do a hypothesis test on the correlation** **Spearman's Test**\\\nH0: The correlation between these two variables is 0\\\nHa: The correlation != 0\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(virg$Sepal.Length, virg$Sepal.Width, method=\"spearman\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in cor.test.default(virg$Sepal.Length, virg$Sepal.Width, method =\n\"spearman\"): Cannot compute exact p-value with ties\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tSpearman's rank correlation rho\n\ndata:  virg$Sepal.Length and virg$Sepal.Width\nS = 11943, p-value = 0.002011\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.4265165 \n```\n:::\n:::\n\n\nThe above output gives us the r value (cor=0.457) AND a p-value for a hypothesis test that the two correlations do not differ. If p\\<0.05 we can reject our H0 and say that the correlation differs from 0. Here, p=0.0008 so we can reject H0 and suggest that we have a significant positive correlation! Rho is similar to r and is this case our correlation coefficient (0.42). It is slightly lower than the r we calculated above.\n\n## Multiple Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris2<-iris[,c(1:4)] #filter iris so we only have the numerical columns!\n\niris_cor<-cor(iris2, method=\"spearman\")\n\niris_cor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1667777    0.8818981   0.8342888\nSepal.Width    -0.1667777   1.0000000   -0.3096351  -0.2890317\nPetal.Length    0.8818981  -0.3096351    1.0000000   0.9376668\nPetal.Width     0.8342888  -0.2890317    0.9376668   1.0000000\n```\n:::\n:::\n\n\nThe above correlation matrix shows r (correlation coefficient) not p values!\n\n**Getting r and p values**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata.rcorr = rcorr(as.matrix(iris2))\nmydata.rcorr #top matrix = r, bottom matrix = p\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\nn= 150 \n\n\nP\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length              0.1519      0.0000       0.0000     \nSepal.Width  0.1519                   0.0000       0.0000     \nPetal.Length 0.0000       0.0000                   0.0000     \nPetal.Width  0.0000       0.0000      0.0000                  \n```\n:::\n:::\n\n\n**Plotting our correlations**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot(iris_cor)\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Categorical correlations (Chi-Square)\n\nA Chi-square test is a statistical test used to determine if two categorical variables have a significant correlation between them. These two variables should be selected from the same population. An example - Is the color of a thing red or green? Is the answer to a simple question yes or no?\\\n\\\n**Data format** Technically, a chi-square test is done on data that are in a contingency table (contains columns (variables) in which numbers represent counts. For example, here is a contingency table of household chore data (exciting)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchore <- read.delim(\"http://www.sthda.com/sthda/RDoc/data/housetasks.txt\", row.names=1)\nchore\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Wife Alternating Husband Jointly\nLaundry     156          14       2       4\nMain_meal   124          20       5       4\nDinner       77          11       7      13\nBreakfeast   82          36      15       7\nTidying      53          11       1      57\nDishes       32          24       4      53\nShopping     33          23       9      55\nOfficial     12          46      23      15\nDriving      10          51      75       3\nFinances     13          13      21      66\nInsurance     8           1      53      77\nRepairs       0           3     160       2\nHolidays      0           1       6     153\n```\n:::\n:::\n\n\n**H0** = The row and column data of the contingency table are independent (no relationship) **Ha**= Row and column variables are dependent (there is a relationship between them)\n\n**The test**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchorechi<-chisq.test(chore)\nchorechi\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  chore\nX-squared = 1944.5, df = 36, p-value < 2.2e-16\n```\n:::\n:::\n\n\nThis result demonstrates that there is a significant association between the columns and rows in the data (they are dependent).\\\n\n**A second example**\n\nLet's try to assess correlation between two categorical variables in a dataframe we know! We will use mtcars\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n:::\n\n```{.r .cell-code}\n#make a contingency table\ncartab<-table(mtcars$carb, mtcars$cyl)\n\nchisq.test(cartab)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in chisq.test(cartab): Chi-squared approximation may be incorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  cartab\nX-squared = 24.389, df = 10, p-value = 0.006632\n```\n:::\n\n```{.r .cell-code}\n#note that we don't NEED to make the table. We can just do this\nchisq.test(mtcars$carb, mtcars$cyl)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in chisq.test(mtcars$carb, mtcars$cyl): Chi-squared approximation may\nbe incorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  mtcars$carb and mtcars$cyl\nX-squared = 24.389, df = 10, p-value = 0.006632\n```\n:::\n:::\n\n\nBoth tests above are the same (just two options for you). We see that p\\<0.05, thus we have evidence to reject H0 and suggest that carb and cyl are dependent / correlated.\n:::\n\n# **3. (Simple) Linear Regression**\n\n::: panel-tabset\nA linear regression essentially compares the correlation of one variable with another. The closer the relationship is to 1:1 (a diagonal line at 45 degrees from the x and y axis) the more correlated the two variables are. Does correlation imply causation? NO, it does not. But this type of analysis driven by hypotheses can help us seek causation/ mechanisms and statistically assess relationships.\n\nLet's take a look at a simple linear regression. To do this, we will use the lm() function in R. The syntax should always be reponsevariable \\~ explanatoryvariable We will do this with the iris data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1<-lm(Sepal.Length ~ Petal.Length, data=iris)\nsummary(lm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76,\tAdjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe above table produces estimates for the slope and intercept of the line.\n\nAt the bottom we see R2 values (multiple and adjusted. We usually use adjusted Rsquared). We also see an overall p-value for our linear regression model (H0= slope of our regression line = 0).\n\n## plotting a regression line\n\nIt is very easy to make a regression line in ggplot. We can plot our scatterplot as we normally would and then we add the regression line using the geom_smooth() argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(iris, aes(x=Petal.Length, y=Sepal.Length))+\n  geom_point()+\n  geom_smooth(method='lm')+\n  theme_classic()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\\\nThe blue line represents our regression line (y\\~x). The gray around the line is the SE. We can add SE=FALSE to our geom_smooth() to turn that off:\n\ngeom_smooth(method='lm', SE=FALSE)\n\n## **Assumptions**\n\nLinear regressions have 4 assumptions:\n\n**1.)** Linearity of the data: We assume the relationship between predictor (x) and outcome/dependent variable (y) is approx. linear. At each value of X there is a population of possible Y-values whose mean lies on the regression line.\\\n\n**2.)** Normality of residuals: The residual error are assumed to be normally distributed. In other words: at each value of X, the distribution of possible Y values is normal\\\n\n**3.)** Homogeneity of residual variance (homoscedasticity): We assume residual variance is approx. constant. In other words: the variance of Y values is the same at all values of X\\\n**4.)** Independence of residual error terms: At each value of X, the Y-measurements represent a random sample from the population of possible Y values.\\\n\nWe can also make a residual plot to check some of our assumptions. **Residuals** measure the scatter of points above or below the least-squares regression line. When we calculate the residuals for a linear regression and plot them, y=0 is the least squares line. Residuals essentially represent the distance between each point and the linear regression line we see in our regression graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals(lm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          1           2           3           4           5           6 \n 0.22090540  0.02090540 -0.13820238 -0.31998683  0.12090540  0.39822871 \n          7           8           9          10          11          12 \n-0.27909460  0.08001317 -0.47909460 -0.01998683  0.48001317 -0.16087906 \n         13          14          15          16          17          18 \n-0.07909460 -0.45641792  1.00268985  0.78001317  0.56179762  0.22090540 \n         19          20          21          22          23          24 \n 0.69822871  0.18001317  0.39822871  0.18001317 -0.11552569  0.09822871 \n         25          26          27          28          29          30 \n-0.28355574  0.03912094  0.03912094  0.28001317  0.32090540 -0.26087906 \n         31          32          33          34          35          36 \n-0.16087906  0.48001317  0.28001317  0.62090540 -0.01998683  0.20268985 \n         37          38          39          40          41          42 \n 0.66179762  0.02090540 -0.43820238  0.18001317  0.16179762 -0.33820238 \n         43          44          45          46          47          48 \n-0.43820238  0.03912094  0.01644426 -0.07909460  0.13912094 -0.27909460 \n         49          50          51          52          53          54 \n 0.38001317  0.12090540  0.77146188  0.25324634  0.58967743 -0.44229252 \n         55          56          57          58          59          60 \n 0.31235411 -0.44675366  0.07146188 -0.75604693  0.41235411 -0.70140030 \n         61          62          63          64          65          66 \n-0.73783139 -0.12407698  0.05770748 -0.12853812 -0.17872361  0.59413856 \n         67          68          69          70          71          72 \n-0.54675366 -0.18318475  0.05324634 -0.30140030 -0.36943035  0.15770748 \n         73          74          75          76          77          78 \n-0.01032257 -0.12853812  0.33503079  0.49413856  0.53056965  0.34878520 \n         79          80          81          82          83          84 \n-0.14675366 -0.03783139 -0.36050807 -0.31961584 -0.10140030 -0.39210703 \n         85          86          87          88          89          90 \n-0.74675366 -0.14675366  0.47146188  0.19413856 -0.38318475 -0.44229252 \n         91          92          93          94          95          96 \n-0.60586144 -0.08764589 -0.14229252 -0.65604693 -0.42407698 -0.32407698 \n         97          98          99         100         101         102 \n-0.32407698  0.13503079 -0.43337025 -0.28318475 -0.46013708 -0.59210703 \n        103         104         105         106         107         108 \n 0.38075515 -0.29656817 -0.17835262  0.59450955 -1.24675366  0.41718624 \n        109         110         111         112         113         114 \n 0.02164738  0.39897069  0.10789297 -0.07389149  0.24432406 -0.65121480 \n        115         116         117         118         119         120 \n-0.59210703 -0.07389149 -0.05567594  0.65361733  0.57183287 -0.35121480 \n        121         122         123         124         125         126 \n 0.26253960 -0.71032257  0.65361733 -0.01032257  0.06253960  0.43986292 \n        127         128         129         130         131         132 \n-0.06943035 -0.21032257 -0.19656817  0.52164738  0.59897069  0.97629401 \n        133         134         135         136         137         138 \n-0.19656817 -0.09210703 -0.49656817  0.89897069 -0.29656817 -0.15567594 \n        139         140         141         142         143         144 \n-0.26943035  0.38521629  0.10343183  0.50789297 -0.59210703  0.08075515 \n        145         146         147         148         149         150 \n 0.06253960  0.26700074 -0.05121480  0.06700074 -0.31478371 -0.49210703 \n```\n:::\n\n```{.r .cell-code}\nggplot(lm1, aes(x=.fitted, y=.resid))+\n  geom_point()+\n  geom_hline(yintercept=0, linetype='dashed')+\n  labs(x='Petal Legnth', y='Residuals')+\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\\\nIf assumptions of normality and equal variance are met, a residual plot should have: - A roughly symmetric cloud of points above and below the horizontal line at 0 with a higher density of points close to the line ran away from it.\\\n- Little noticeable curvature as we move from left to right\\\n- Approx. equal variance of points above and below the line at all values of X\\\n\\\n\nThe residual plot above shows meets all assumptions, though this analysis is somewhat subjective.\n\n**An alternative assumption check** I think it is easier to do a more comprehensive visual check with the performance package in R. We can easily visually check the first 3 assumptions using check_model(). Assumption 4 requires us to think about experimental design.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1<-lm(Sepal.Length ~ Petal.Length, data=iris)\n\ncheck_model(lm1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n```\n:::\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\\\nUsing the plots above, we can check 3 / 4 of our assumptions and look for influential observations/outliers. The plots even tell us what to look for on them! This is a bit simpler than trying to analyze the residual plot.\\\nAs with the residual plot, this analysis of assumptions is somewhat subjective. That is ok.\n\n## **when data are not linear**\n\nSometimes the relationship between two variables is not linear! There are many types of common relationships including logarithmic and exponential. We can often visualize these relationships and **Transform** our data to make them linear with some simple math.\n\nLet's look at an example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Loblolly)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGrouped Data: height ~ age | Seed\n   height age Seed\n1    4.51   3  301\n15  10.89   5  301\n29  28.72  10  301\n43  41.74  15  301\n57  52.70  20  301\n71  60.92  25  301\n```\n:::\n\n```{.r .cell-code}\np1<-ggplot(Loblolly, aes(x=age, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='original')\n#this is roughly logarithmic in shape\n\nlob<-Loblolly\nlob$age2<-log(lob$age)\n\np2<-ggplot(lob, aes(x=age2, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='log transformed')\n\nlob$age3=(lob$age2)^2\np3<-ggplot(lob, aes(x=age3, y=height))+\n  geom_point()+\n  geom_smooth()+\n  geom_smooth(method='lm', linetype='dashed', color='firebrick')+\n  theme_classic()+\n  labs(title='squared')\n\np1/p2/p3\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nHere we can see that the transformation was fairly trivial (the data were close enough to a straight line already). BUT, technically, the first plot shows a logarithmic trend. We can transform one of the variables to generate a more linear trend. We can guess a transformation and check it with graphs or we can use our knowledge of mathematical relationships to understand how we might make our relationship more linear.\n\n## **Linear Regression with categorical variables**\n\nWe can look at mtcars this time...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n:::\n:::\n\n\nNow, I want to hypothesize that there will be no effect of cylinder on horsepower (this is called a \"null hypothesis\"). We've seen similar hypothesis before in our ANOVA.\n\nFirst, let's make cylinder a factor and plot a boxplot so we can see whether there may be a trend here...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars$cyl1=as.factor(mtcars$cyl)\n\nggplot(mtcars, aes(x=cyl1, y=hp))+\n         geom_boxplot()+\n         theme_bw()\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\\\nI think it is safe to say we see what we might suspect to be a linear(ish) relationship between cyl and hp, where hp increases as cyl increases. What do you think?\n\nNow, let's do some stats on this.\n\n## **Run the lm**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmhp<-lm(hp~cyl1, data = mtcars)\nsummary(lmhp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = hp ~ cyl1, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-59.21 -22.78  -8.25  15.97 125.79 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    82.64      11.43   7.228 5.86e-08 ***\ncyl16          39.65      18.33   2.163   0.0389 *  \ncyl18         126.58      15.28   8.285 3.92e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 37.92 on 29 degrees of freedom\nMultiple R-squared:  0.7139,\tAdjusted R-squared:  0.6941 \nF-statistic: 36.18 on 2 and 29 DF,  p-value: 1.319e-08\n```\n:::\n:::\n\n\nThis time we used a categorical x variable, which makes things a little more interesting. In the coefficients table this time we see cyl = 6 and cyl =8 represented as well as \"intercept.\" R takes the categorical variables and places them in alpha numeric order in these tables. So \"intercept\" is actually cyl=4. The \"estimate\" tells us the effect size of each category relative to \"intercept.\" SO, the mean of cyl=4 should be 82.64 (check the boxplot above to confirm). The mean of cyl=6 is not 39.65, but is actually 39.65 higher than mean of cyl=4 (82.64 + 39.65 = 132.29, which checks out). The p-values associated with each of the coefficients test the null hypothesis that each coefficient has no effect. A p \\<0.05 indicates that the coefficient is likely to be meaningful in the model (changes in the predictor's value are related to changes in the response value). </br>\n\nFurther down, we see an R-squared of nearly 0.70, which is very good evidence of a linear relationship (70% of the variance in y can be explained by x!). The p-value is very nearly 0.00, which indicates a significant linear correlation.\n\n## **Check assumptions!**\n\n\n::: {.cell meesage='false'}\n\n```{.r .cell-code}\ncheck_model(lmhp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n```\n:::\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\\\nHere we see some concern about Homoscedasticity and homogeneity of variance. We can probably still assume our model is reliable, but we may want to be careful. We learned ways to numerically assess this last week, but again, with high enough sample size, this won't be an issue. Here, I would suggest that n is too small, so if this were a real statistical test we would have limitations to discuss.\\\n\nRemember our hypothesis (null) was: \"There will be no effect of cylinder on horsepower.\" We are able to reject this null hypothesis and suggest that indeed horsepower increases as cylinder increases. We might also add caveats that homoscedasticity was not confirmed due to low sample size, but the result seems clear enough that this likely doesn't matter.\\\n:::\n\n## **4. t-test**\n\n# Additional Tutorials and Resources for t-tests\n\n[t-tests](https://statistics.berkeley.edu/computing/r-t-tests)\n\n::: panel-tabset\n## **A note on statistics and experimental design**\n\nStatistics is a complex field with a long history. We could spend an entire course or even an entire career focusing on the intricate details of statistical decisions and ideas. We've already spent some time on this! I want you to have the statistical grounding necessary to plan your experiments and analyze your data. For biologists, statistics are a tool we can leverage to perform the best possible experiments and test our hypotheses. The T-test is the start of our stats journey. It's a simple test and one that you may not use often, but the theory behind it sets the stage for what is to come!\n\n\\\n\n## **T-test theory**\n\nThe t-test (or students' t-test) is a basic statistical test used to assess whether or not the means of two groups are different from one another. In this test, the null hypothesis is that the two means are equal (or that there is no difference between the two means).\n\n**A t-test should only be used if the following assumptions are met:**\\\n**1.)** the two distributions whose means we are comparing must be **normally distributed**\\\n**2.)** The variances of the two groups must be **equal**\\\n\n**Generate example data**\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris2<-iris %>%\n  filter(Species != 'setosa') %>%\n  droplevels() #removes the empty levels so when we check levels below we only get the ones that are still in the data!\n\n#check levels to make sure we only have 2 species!\nhead(iris2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          7.0         3.2          4.7         1.4 versicolor\n2          6.4         3.2          4.5         1.5 versicolor\n3          6.9         3.1          4.9         1.5 versicolor\n4          5.5         2.3          4.0         1.3 versicolor\n5          6.5         2.8          4.6         1.5 versicolor\n6          5.7         2.8          4.5         1.3 versicolor\n```\n:::\n\n```{.r .cell-code}\nlevels(iris2$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"versicolor\" \"virginica\" \n```\n:::\n:::\n\n\nWe will use these data for our examples today. T-test requires *only* 2 groups/populations. We will assess the alternative hypothesis that one of our numerical variables (sepal length, sepal width, petal length, or petal width) differs by species.\n\nBut first, we must **test our assumptions**\n\n## **Assumption 1.) Assessing normality**\n\n*Method 1: the Shapiro-Wilk Test* If p \\< 0.05 then the distribution is significantly different from normal.\n\nStep 1: we need to create separate data frames for each species to assess normality of each variable by species!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nversi<-iris2 %>%\n  filter(Species=='versicolor') %>%\n  droplevels()\n\nvirg<-iris2 %>%\n  filter(Species=='virginica') %>%\n  droplevels()\n```\n:::\n\n\n\\\n\nStep 2: We can run our shapiro-wilk tests on each variable if we'd like\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(versi$Petal.Length) #this is normally distributed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  versi$Petal.Length\nW = 0.966, p-value = 0.1585\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(versi$Petal.Width) # this is not\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  versi$Petal.Width\nW = 0.94763, p-value = 0.02728\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(versi$Sepal.Length) #normal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  versi$Sepal.Length\nW = 0.97784, p-value = 0.4647\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(versi$Sepal.Width) #normal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  versi$Sepal.Width\nW = 0.97413, p-value = 0.338\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(virg$Petal.Length) #normal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  virg$Petal.Length\nW = 0.96219, p-value = 0.1098\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(virg$Petal.Width) #normal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  virg$Petal.Width\nW = 0.95977, p-value = 0.08695\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(virg$Sepal.Length) #normal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  virg$Sepal.Length\nW = 0.97118, p-value = 0.2583\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(virg$Sepal.Width) #normal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  virg$Sepal.Width\nW = 0.96739, p-value = 0.1809\n```\n:::\n:::\n\n\n\\\n*Method 2: Visualization*\n\nExplore the following visualizations. Do you see clear evidence of normality?\n\n\n::: {.cell}\n\n```{.r .cell-code}\na1<-ggplot(data=iris2, aes(Petal.Length, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\na2<-ggplot(data=iris2, aes(x=Petal.Length, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\na1/a2 #compare the visualizations (they are of the same data)- do we see normality here?\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPicking joint bandwidth of 0.206\n```\n:::\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb1<-ggplot(data=iris2, aes(Petal.Width, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nb2<-ggplot(data=iris2, aes(x=Petal.Width, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nb1/b2 #compare the visualizations (they are of the same data)- do we see normality here?\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPicking joint bandwidth of 0.0972\n```\n:::\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nc1<-ggplot(data=iris2, aes(Sepal.Width, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nc2<-ggplot(data=iris2, aes(x=Sepal.Width, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nc1/c2 #compare the visualizations (they are of the same data)- do we see normality here?\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPicking joint bandwidth of 0.122\n```\n:::\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd1<-ggplot(data=iris2, aes(Sepal.Length, fill=Species))+\n  geom_histogram(binwidth = 0.3)+ \n  facet_wrap(~Species)+\n  theme_classic()+\n  scale_fill_aaas()\n\nd2<-ggplot(data=iris2, aes(x=Sepal.Length, y=Species, fill=Species))+\n  geom_density_ridges()+ #makes a smooth density curve instead of a histogram!\n  theme_classic()+\n  scale_fill_aaas()\n\nd1/d2 #compare the visualizations (they are of the same data)- do we see normality here?\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPicking joint bandwidth of 0.21\n```\n:::\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## **Assumption 2.) Assessing equal variance**\n\nAKA homogeneity of variance\\\n\n**Methods 1: F-test** We will use the **F-Test** to compare the variance of two populations. This can only be used with *2* populations and is thus only useful when we run a t-test.\n\nH0 for an F-test is: The variances of the two groups are equal.\\\nHa: The variances are different\\\np\\<0.05 allows us to reject the null (H0) and suggests that the variances are different\\\n\\\n**note:** The F-test assumes our data are already normal! You should not run it on non-normal data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#we use var.test to run an F-test\nf1<- var.test(Petal.Length ~ Species, data=iris2)\nf1 # p>0.05, so we fail to reject H0 (the variances are likely equal)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tF test to compare two variances\n\ndata:  Petal.Length by Species\nF = 0.72497, num df = 49, denom df = 49, p-value = 0.2637\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.411402 1.277530\nsample estimates:\nratio of variances \n         0.7249678 \n```\n:::\n\n```{.r .cell-code}\nf2<- var.test(Petal.Width ~ Species, data=iris2)\nf2 # p<0.05, so we reject H0 (variances are likely different)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tF test to compare two variances\n\ndata:  Petal.Width by Species\nF = 0.51842, num df = 49, denom df = 49, p-value = 0.02335\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2941935 0.9135614\nsample estimates:\nratio of variances \n         0.5184243 \n```\n:::\n\n```{.r .cell-code}\nf3<- var.test(Sepal.Length ~ Species, data=iris2)\nf3 # p>0.05, so we fail to reject H0 (the variances are likely equal)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tF test to compare two variances\n\ndata:  Sepal.Length by Species\nF = 0.65893, num df = 49, denom df = 49, p-value = 0.1478\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3739257 1.1611546\nsample estimates:\nratio of variances \n         0.6589276 \n```\n:::\n\n```{.r .cell-code}\nf4<- var.test(Sepal.Width ~ Species, data=iris2)\nf4 # p>0.05, so we fail to reject H0 (the variances are likely equal)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tF test to compare two variances\n\ndata:  Sepal.Width by Species\nF = 0.94678, num df = 49, denom df = 49, p-value = 0.849\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.5372773 1.6684117\nsample estimates:\nratio of variances \n         0.9467839 \n```\n:::\n:::\n\n\n\\\n**Method 2: Levene Test**\\\nA more flexible test of homogeneity of variance is the Levene Test. It can be used to compare the variance of many populations (not just 2) and is more flexible than the F-test, so it can be used even if the normality assumption is violated.\\\n**this is the most commonly used test for homogeneity of variance**\\\n**leveneTest() is in the car package in R!**\\\n\nN0: Variances of all populations are equal\\\np\\<0.05 allows us to reject H0\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl1<- leveneTest(Petal.Length ~ Species, data=iris2)\nl1 # p>0.05, so we fail to reject H0 (the variances are likely equal)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  1.0674 0.3041\n      98               \n```\n:::\n\n```{.r .cell-code}\nl2<- leveneTest(Petal.Width ~ Species, data=iris2)\nl2 # p<0.05, so we reject H0 (variances are likely different)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(>F)  \ngroup  1  6.5455 0.01205 *\n      98                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nl3<- leveneTest(Sepal.Length ~ Species, data=iris2)\nl3 # p>0.05, so we fail to reject H0 (the variances are likely equal)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  1.0245 0.3139\n      98               \n```\n:::\n\n```{.r .cell-code}\nl4<- leveneTest(Sepal.Width ~ Species, data=iris2)\nl4 # p>0.05, so we fail to reject H0 (the variances are likely equal)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  0.0873 0.7683\n      98               \n```\n:::\n:::\n\n\n\\\n**Method 3: Visualization**\\\nSince p-values are more like guidelines, we also want to visualize our data to assess homogeneity of variance. We can do that in several ways. You might already have some ideas about this! In general, it seems smart to display the raw data as points and as boxplots. Let's start there!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nv1.1<-ggplot(data=iris2, aes(x=Species, y=Petal.Length, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv1.2<-ggplot(data=iris2, aes(x=Species, y=Petal.Length, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv1.1+v1.2\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nv2.1<-ggplot(data=iris2, aes(x=Species, y=Petal.Width, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv2.2<-ggplot(data=iris2, aes(x=Species, y=Petal.Width, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv2.1+v2.2\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nv3.1<-ggplot(data=iris2, aes(x=Species, y=Sepal.Width, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv3.2<-ggplot(data=iris2, aes(x=Species, y=Sepal.Width, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv3.1+v3.2\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nv4.1<-ggplot(data=iris2, aes(x=Species, y=Sepal.Length, color=Species))+\n  geom_point()+\n  theme_classic()+\n  scale_color_aaas()\n\nv4.2<-ggplot(data=iris2, aes(x=Species, y=Sepal.Length, color=Species))+\n  geom_boxplot()+\n  theme_classic()+\n  scale_color_aaas()\n\nv4.1+v4.2\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n## **When can we ignore assumptions?**\n\nWe can if our sample sizes are large. If n is small, we should not ignore this assumption. There are alternatives to dealing with normality that we can discuss in the ANOVA section (such as transforming the data)\n\n[For more info on that](https://thestatsgeek.com/2013/09/28/the-t-test-and-robustness-to-non-normality/)\n\nWe can also ignore the equal variance requirement if we use the Welch t-test (default in R)\\\n\n## **A basic T-test in R**\n\nFinally, let's do some T-tests!\\\n\nH0: No difference between the means of the 2 populations p\\<0.05 allows us to reject this H0 (indicating a likely difference)\n\n**Step 1:** Calculate means and error and plot!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeaniris<-iris2 %>%\n  group_by(Species) %>%\n  dplyr::summarise(meanpl=mean(Petal.Length), sdpl=sd(Petal.Length), n=n(), sepl=sdpl/sqrt(n), meanpw=mean(Petal.Width), sdpw=sd(Petal.Width), n=n(), sepw=sdpw/sqrt(n), meansl=mean(Sepal.Length), sdsl=sd(Sepal.Length), n=n(), sesl=sdpl/sqrt(n), meansw=mean(Sepal.Width), sdsw=sd(Sepal.Width), n=n(), sesw=sdsw/sqrt(n))\n\nmeaniris\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 14\n  Species    meanpl  sdpl     n   sepl meanpw  sdpw   sepw meansl  sdsl   sesl\n  <fct>       <dbl> <dbl> <int>  <dbl>  <dbl> <dbl>  <dbl>  <dbl> <dbl>  <dbl>\n1 versicolor   4.26 0.470    50 0.0665   1.33 0.198 0.0280   5.94 0.516 0.0665\n2 virginica    5.55 0.552    50 0.0780   2.03 0.275 0.0388   6.59 0.636 0.0780\n# ℹ 3 more variables: meansw <dbl>, sdsw <dbl>, sesw <dbl>\n```\n:::\n:::\n\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1<-ggplot(meaniris, aes(x=Species, y=meanpl, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meanpl-sepl, ymax=meanpl+sepl), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Petal Length')\n\np2<-ggplot(meaniris, aes(x=Species, y=meanpw, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meanpw-sepw, ymax=meanpw+sepw), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Petal Width')\n\np3<-ggplot(meaniris, aes(x=Species, y=meansl, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meansl-sesl, ymax=meansl+sesl), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Sepal Length')\n\np4<-ggplot(meaniris, aes(x=Species, y=meansw, color=Species))+\n  geom_point()+\n  geom_errorbar(aes(x=Species, ymin=meansw-sesw, ymax=meansw+sesw), width=0.2)+\n  scale_color_aaas()+\n  theme_classic()+\n  labs(title='Sepal Width')\n\n(p1+p2)/(p3+p4)\n```\n\n::: {.cell-output-display}\n![](cor_reg_chi_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n**Does Petal Length differ by species?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1<-t.test(data=iris2, Petal.Length~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt1 #p<0.05 suggests that there is a significant difference in petal length between species\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  Petal.Length by Species\nt = -12.604, df = 95.57, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n```\n:::\n:::\n\n\n\\\nOur p\\<0.05 suggests that there is a significant effect of species on petal length (petal length differs by species). BUT, do we get a clear explanation of which group is higher or lower? Look at the Welch T-test output and you can see the means! You can also use the graph we made to visualize this!\n\n**Does Petal Width differ by species?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt2<-t.test(data=iris2, Petal.Width~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  Petal.Width by Species\nt = -14.625, df = 89.043, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.7951002 -0.6048998\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   1.326                    2.026 \n```\n:::\n:::\n\n\n\\\n**Does Sepal Width differ between species?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt3<-t.test(data=iris2, Sepal.Width~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  Sepal.Width by Species\nt = -3.2058, df = 97.927, p-value = 0.001819\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.33028364 -0.07771636\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   2.770                    2.974 \n```\n:::\n:::\n\n\n\\\n**Does Sepal Length differ between species?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt4<-t.test(data=iris2, Sepal.Length~Species, alternative='two.sided', var.equal=FALSE) #two.sided and var.equal= FALSE are default, so we don't have to list them. BUt, we can also change them (as I will show later)\n\nt4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  Sepal.Length by Species\nt = -5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -0.8819731 -0.4220269\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   5.936                    6.588 \n```\n:::\n:::\n\n\nSO, when is a t-test actually useful and when isn't it? We use a T-test **ONLY** when we want to compare two means / two populations. If we have more than 2 groups, a T-test is not appropriate! Instead, we need to use an analysis of variance (ANOVA) or possibly something more complex!\n:::\n",
    "supporting": [
      "cor_reg_chi_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}